# Development Documentation of `Trchime` 

## Tensor

In `Trchime`,  the basic unit that constitutes the arithmetic and automatic differentiation system is `Tensor` .

You can explicitly declare a `Tensor` as you would with `Pytorch`. 

``` python
import trchime as tce
a = tce.Tensor(1, dtype = tce.int32)
b = tce.Tensor([[1, 2],
                [3, 4]], dtype = tce.int32)
c = a + b
print(a)
print(b)
print(c)
```

result:

``` python
Tensor(
1, requires_grad=False, dtype=int32)
Tensor(
[[1 2]
 [3 4]], requires_grad=False, dtype=int32)
Tensor(
[[2 3]
 [4 5]], requires_grad=False, dtype=int32)
```

When initializing a tensor, you can specify the value of some optional parameters:

> 1. dtype: optional, types of data in Tensor. If not given, default value will be determined by the data type.
>
> 2. requires_grad: optional, boolean, whether mark tensor as trainable. If not given, default value is False.
>
> 3. depends_on: optional, List[Dependency].  This option is automatically generated by `Trchime ` when generating the computational graph.

Tensor objects are derived from two subclasses so that you can initialize your data: `Variable`、`Constant`.

``` python
from trchime import Variable, Constant
a = Variable([1, 2, 3])
b = Constant(3)
print(a + b)

a = Variable(shape = (3, 4))  
# You can initialize tensor randomly by sepecifying shape.
# Create a tensor of the given shape and populate it with random samples 
# from a standard normal distribution.
b = Constant(shape = ())
print(a + b)
```

result:

``` python
Tensor(
[4 5 6], requires_grad=True, dtype=int32)
Tensor(
[[ 0.1356221   0.04475528 -0.07826413 -0.10148723]  # randomly
 [ 0.19935779 -0.02828883  0.02858137 -0.01568915]  # randomly
 [-0.15714294  0.07856039  0.21736954  0.11458201]], requires_grad=True, dtype=float64)
```

## Operator

Like `Numpy`, `Tchime` provides many operations for working with tensor objects.

You can perform common mathematical operations through `Tensor` and `Operator`, which constitute a relatively complete algebraic system. In addition, the tensor-based dynamic computational graph model also includes an automatic differentiation library.

It is worth noting that only tensor marked as trainable will have their gradients computed by `Trchime`. 

``` python
import trchime as tce
a = tce.Tensor([[1, 2, 3],
                [2, 3, 4]], requires_grad = True)  # mark your tensor as trainable
b = tce.Tensor([-1, 0, 0.5])
                
c = tce.exp(a) + tce.sin(b) / tce.tanh(a)  # some operations for tensor

d = c.sum()
d.backward()

print(d)
print(a.grad)
print(b.grad)  # b is non-trainable tensor
```

result:

``` python
Tensor(
111.2494220755271, requires_grad=True, dtype=float64)
Tensor(
[[ 3.32755871  7.3890561  20.08075976]
 [ 7.45302626 20.08553692 54.59750628]], requires_grad=False, dtype=float64)
None
```

In `Trchime`, the operation functions in tensor are mainly stored in the `multitensor` and `func` packages, where `multitensor` contains common ops for arrays. The basic functions for deep learning are collected in the `func` .

### Multitensor



### func

- `tanh`:

  
  Here implements the tanh(hyperbolic tangent) function for tensors.
  
  Parameter: t: 'Tensorable'
  Return: Tensor: value of tanh(t)
  
  Examples:
  >>> a = tce.Tensor([[1, 2],
                      [3, 4]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.tanh(a)
  >>> b
  Tensor(
  [[0.76159416 0.96402758]
   [0.99505475 0.9993293 ]], requires_grad=True, dtype=float64)
  >>> x = tce.tanh(3)
  >>> x
  Tensor(
  0.9950547536867305, requires_grad=False, dtype=float64)
  >>> y = tce.trange(6, 9)
  >>> print(tce.tanh(y))
  Tensor(
  [0.99998771 0.99999834 0.99999977], requires_grad=False, dtype=float64)
  
- `sin`:

  Here implements the sin(sine) function for tensors.
  Parameter: t: 'Tensorable' Return: Tensor: value of sin(t)
  Examples: >>> a = tce.Tensor([[1, 2],
   [3, 4]], requires_grad = True, dtype = tce.int32)

  >>> b = tce.sin(a)
  >>> b
  >>> Tensor(
  >>> [[ 0.84147098  0.90929743]
  >>>  [ 0.14112001 -0.7568025 ]], requires_grad=True, dtype=float64)
  >>> x = tce.tanh(1.57)
  >>> x
  >>> Tensor(
  >>> 0.9999996829318346, requires_grad=False, dtype=float64)
  >>> y = tce.trange(6, 9)
  >>> print(tce.tanh(y))
  >>> Tensor(
  >>> [-0.2794155   0.6569866   0.98935825], requires_grad=False, dtype=float64)

- `cos`:

  Here implements the cos(cosine) function for tensors.
  Parameter: t: 'Tensorable' Return: Tensor: value of cos(t)
  Examples: >>> a = tce.Tensor([[1, 2],
   [3, 4]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.cos(a)
  >>> b
  >>> Tensor(
  >>> [[ 0.54030231 -0.41614684]
  >>>  [-0.9899925  -0.65364362]], requires_grad=True, dtype=float64)
  >>> x = tce.cos(3.14)
  >>> x
  >>> Tensor(
  >>> -0.9999987317275395, requires_grad=False, dtype=float64)
  >>> y = tce.trange(6, 9)
  >>> print(tce.cos(y))
  >>> Tensor(
  >>> [ 0.96017029  0.75390225 -0.14550003], requires_grad=False, dtype=float64)

- `tanh`:

  Here implements the tan(tangent) function for tensors.
  Parameter: t: 'Tensorable' Return: Tensor: value of tan(t)
  Examples: >>> a = tce.Tensor([[1, 2],
   [3, 4]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.tan(a)
  >>> b
  >>> Tensor(
  >>> [[ 1.55740772 -2.18503986]
  >>>  [-0.14254654  1.15782128]], requires_grad=True, dtype=float64)
  >>> x = tce.tan(1.57)
  >>> x
  >>> Tensor(
  >>> 1255.7655915007897, requires_grad=False, dtype=float64)
  >>> y = tce.trange(6, 9)
  >>> print(tce.tan(y))
  >>> Tensor(
  >>> [-0.29100619  0.87144798 -6.79971146], requires_grad=False, dtype=float64)

- `exp`:

  Here implements the exp(exponential) function for tensors.
  Parameter: t: 'Tensorable' Return: Tensor: value of exp(t)
  Examples: >>> a = tce.Tensor([[1, 2],
   [3, 4]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.exp(a)
  >>> b
  >>> Tensor(
  >>> [[ 2.71828183  7.3890561 ]
  >>>  [20.08553692 54.59815003]], requires_grad=True, dtype=float64)
  >>> x = tce.exp(tce.log(2))
  >>> x
  >>> Tensor(
  >>> 2.0, requires_grad=False, dtype=float64)
  >>> y = tce.trange(6, 9)
  >>> print(tce.exp(y))
  >>> Tensor(
  >>> [ 403.42879349 1096.63315843 2980.95798704], requires_grad=False, dtype=float64)

- `log`:

  Here implements the log(logarithmic) function for tensors.
  Parameter: t: 'Tensorable' Return: Tensor: value of log(t)
  Examples: >>> a = tce.Tensor([[1, 2],
   [3, 4]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.log(a)
  >>> b
  >>> Tensor(
  >>> [[0.         0.69314718]
  >>>  [1.09861229 1.38629436]], requires_grad=True, dtype=float64)
  >>> x = tce.log(tce.exp(6))
  >>> x
  >>> Tensor(
  >>> 6.0, requires_grad=False, dtype=float64)
  >>> y = tce.trange(6, 9)
  >>> print(tce.log(y))
  >>> Tensor(
  >>> [1.79175947 1.94591015 2.07944154], requires_grad=False, dtype=float64)

- `sigmoid`:

  Here implements the sigmoid function for tensors.
  Parameter: t: 'Tensorable' Return: Tensor: value of sigmoid(t) = 1 / (1 + exp(-t))
  Examples: >>> a = tce.Tensor([[1, 2],
   [3, 4]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.sigmoid(a)
  >>> b
  >>> Tensor(
  >>> [[0.73105858 0.88079708]
  >>>  [0.95257413 0.98201379]], requires_grad=True, dtype=float64)
  >>> x = tce.sigmoid(tce.log(6))
  >>> x
  >>> Tensor(
  >>> 0.8571428571428571, requires_grad=False, dtype=float64)
  >>> y = tce.trange(6, 9)
  >>> print(tce.sigmoid(y))
  >>> Tensor(
  >>> [0.99752738 0.99908895 0.99966465], requires_grad=False, dtype=float64)

- `ReLu`:

  Here implements the ReLU(Rectified Linear Units) function for tensors.
  Parameter: t: 'Tensorable' Return: Tensor: value of ReLU(x) = max(0, t)
  Examples: >>> a = tce.Tensor([[-1, -2],
   [1, 2]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.ReLU(a)
  >>> b
  >>> Tensor(
  >>> [[0 0]
  >>>  [1 2]], requires_grad=True, dtype=int32)
  >>> x = tce.ReLU(0)
  >>> x
  >>> Tensor(
  >>> 0, requires_grad=False, dtype=int32)
  >>> y =  tce.log(2)
  >>> print(tce.ReLU(y))
  >>> Tensor(
  >>> 0.6931471805599453, requires_grad=False, dtype=float64)

- `Leaky_ReLU`:

  Here implements the Leaky_ReLU(Leaky Rectified Linear Units) function for tensors.
  Parameter: t: 'Tensorable'
  k: 'float'
  Return: Tensor: value of Leaky-ReLU(t, k) = max(kt, t)
  Examples: >>> a = tce.Tensor([[-1, -3],
   [1, 3]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.Leaky_ReLU(a,0.5)  # t = a, k = 0.5
  >>> b
  >>> Tensor(
  >>> [[-0.5 -1.5 ]  # -1*k = -0.5 > -1 , -3*k = -1.5 > -3
  >>>  [ 1.   3. ]], requires_grad=True, dtype=float64)  # 1*k = 0.5 < 1 , 3*0.5 = 1.5 < 3
  >>> x = tce.Leaky_ReLU(tce.Tensor([1, -1, 0]), -0.5)
  >>> x
  >>> Tensor(
  >>> [ 1.   0.5 -0. ], requires_grad=False, dtype=float64)
  >>> y = tce.log(2)
  >>> print(tce.Leaky_ReLU(y, 30))
  >>> Tensor(
  >>> 20.79441541679836, requires_grad=False, dtype=float64)

- `softmax`:

  Here implements the softmax function for tensors.
  Parameter: t: 'Tensorable'
  axis: 'Integer': Define the axis of t
  Return: Tensor: value of softmax(t, axis = a)
  Examples: >>> a = tce.Tensor([[-1, -3],
   [1, 3]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.softmax(a)
  >>> b
  >>> Tensor(
  >>> [[0.0158422  0.00214401]
  >>>  [0.11705891 0.86495488]], requires_grad=True, dtype=float64)

  ```python
      # tce.exp(-1) = 0.368
      # tce.exp(-3) = 0.050
      # tce.exp(1) = 2.718
      # tce.exp(3) = 20.086
      # sum = 0.368 + 0.050 + 2.718 + 20.086 = 23.222
      # tce.exp(-1)/sum = 0.0158
      # tce.exp(-3)/sum = 0.0021
      # tce.exp(1)/sum = 0.117
      # tce.exp(3)/sum = 0.864
  ```
  
  >>> a = tce.Tensor([[-1, -2],
  >>>        [1, 3]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.softmax(a, axis = 1)  # axis = 1, the softmax starts from the tensors in 1 axis of a
  >>> b
  >>> Tensor(
  >>> [[0.73105858 0.26894142]
  >>>  [0.11920292 0.88079708]], requires_grad=True, dtype=float64)
  
  ```python
  # tce.exp(-1) = 0.368
  # tce.exp(-2) = 0.135
  # sum1 = tce.exp(-1) + tce.exp(-2) = 0.368 + 0.135 = 0.503
  # tce.exp(-1)/sum1 = 0.731
  # tce.exp(-3)/sum2 = 0.268
  ```
  
  >>> a = tce.Tensor([[-1, -2],
  >>>        [1, 3]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.softmax(a, axis = 0)  # axis = 0, the softmax starts from the tensors in 0 axis of a
  >>> b
  >>> Tensor(
  >>> [[0.11920292 0.00669285]
  >>>  [0.88079708 0.99330715]], requires_grad=True, dtype=float64)
  
  ```python
  # tce.exp(-1) = 0.368
  # tce.exp(1) = 2.718
  # sum1 = tce.exp(-1) + tce.exp(1) = 0.368 + 2.718 = 3.086
  # tce.exp(-1)/sum1 = 0.119
  # tce.exp(1)/sum2 = 0.880
  # tce.exp(-2) = 0.135 # tce.exp(3) = 20.086 # sum2 = tce.exp(-2) + tce.exp(3) = 0.135 + 20.086 = 20.221 # tce.exp(1)/sum2 = 0.006 # tce.exp(3)/sum2 = 0.993
  ```

- `maximum`:

  Here implements the maximum function for tensors.
  Parameter: t1, t2: 'Tensorable'
  isnew: 'Bool': decides whether giving result gradient
  Return: Tensor: the max elements in t1, t2, value of maximum(t1, t2) = max(t1, t2) It's combined with broadcastsing operation.
  Examples: >>> a = tce.Tensor([[1,3],
   [-1,1], [5,6]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.Tensor([2])
  >>> tce.maximum(a, b)
  >>> Tensor(
  >>> [[2 3]
  >>>  [2 2]
  >>>  [5 6]], requires_grad=False, dtype=int32)
  >>> a = tce.Tensor([[0,3],
  >>> [-1,1], [5,6]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.Tensor([1,2])
  >>> tce.maximum(a, b)
  >>> Tensor(
  >>> [[1 3]  # 0 < 1, 3 > 2
  >>>  [1 2]  # -1 < 1, 1 < 2
  >>>  [5 6]], requires_grad=False, dtype=int32)  # 5 > 1, 6 > 2
  >>> a = tce.Tensor([[0,3],
  >>>       [-1,1],
  >>>       [5,6]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.Tensor([[1,2],
  >>>        [-2, 8],
  >>>        [10, 5.5]], requires_grad = True, dtype = tce.float32)
  >>> tce.maximum(a, b)
  >>> Tensor(
  >>> [[ 1.  3.]  # 0 < 1, 3 > 2
  >>>  [ -1.  8.]  # -1 > -2, 1 < 8
  >>>  [10.  6.]], requires_grad=False, dtype=float64)  # 5 < 10, 6 > 5.5

- `minimum`:

  Here implements the minimum function for tensors.
  Parameter: t1, t2: 'Tensorable'
  isnew: 'Bool': decides whether giving result gradient
  Return: Tensor: the min elements in t1, t2, value of minimum(t1, t2) = min(t1, t2) It's combined with broadcastsing operation.
  Examples: >>> a = tce.Tensor([[1,3],
   [-1,1], [5,6]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.Tensor([2])
  >>> tce.minimum(a, b)
  >>> Tensor(
  >>> [[ 1  2]
  >>>  [-1  1]
  >>>  [ 2  2]], requires_grad=False, dtype=int32)
  >>> a = tce.Tensor([[0,3],
  >>> [-1,1], [5,6]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.Tensor([1,2])
  >>> tce.minimum(a, b)
  >>> Tensor(
  >>> [[ 0  2]
  >>>  [-1  1]
  >>>  [ 1  2]], requires_grad=False, dtype=int32)
  >>> a = tce.Tensor([[0,3],
  >>>       [-1,1],
  >>>       [5,6]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.Tensor([[1,2],
  >>>        [-2, 8],
  >>>        [10, 5.5]], requires_grad = True, dtype = tce.float32)
  >>> tce.minimum(a, b)
  >>> Tensor(
  >>> [[ 0  2]
  >>>  [-2  1]
  >>>  [ 5  5.5]], requires_grad=False, dtype=float32)

- `one_hot`:

  Here implements the one_hot function for tensors.
  Parameter: t: 'Tensorable'
  depth: optional, 'Integer': the size length of output tensor on_value: optional: the number of those on figure off_value: optional: the number of those off figure param isnew: 'Bool': decides whether giving result gradient
  Return: Tensor: expand t to depth size, It's combined with broadcasting
  Examples
  >>> a = tce.Tensor([1], requires_grad = True, dtype = tce.int32)
  >>> depth = 3
  >>> tce.one_hot(a, depth)  # output: (1, 3)
  >>> Tensor(
  >>> [[0. 1. 0.]], requires_grad=False, dtype=float64)  # the second on_value is 1, else off_value are 0
  >>> a = tce.Tensor([0, 1, 2], requires_grad = True, dtype = tce.int32)
  >>> depth = 3
  >>> tce.one_hot(a, depth)  # output: (3, 3)
  >>> Tensor(
  >>> [[1. 0. 0.]  # one_hot(0)
  >>>  [0. 1. 0.]  # one_hot(1)
  >>>  [0. 0. 1.]], requires_grad=False, dtype=float64)  # one_hot(2)
  >>> a = tce.Tensor([0, 2, 2], requires_grad = True, dtype = tce.int32)
  >>> depth = 3
  >>> tce.one_hot(a, depth)  # output: (3, 3)
  >>> Tensor(
  >>> [[1. 0. 0.]  # one_hot(0)
  >>>  [0. 0. 1.]  # one_hot(2)
  >>>  [0. 0. 1.]], requires_grad=False, dtype=float64)  # one_hot(2)
  >>> a = tce.Tensor([[0, 2, 5],
  >>>       [0, 1, 3]], requires_grad = True, dtype = tce.int32)
  >>> depth = 4
  >>> tce.one_hot(a, depth)  # output: (2, 3, 4)
  >>> Tensor(
  >>> [[[1. 0. 0. 0.]  # one_hot(0)
  >>> [0. 0. 1. 0.]  # one_hot(2)
  >>> [0. 0. 0. 0.]] # one_hot(5)

  [[1. 0. 0. 0.] # one_hot(0)
  [0. 1. 0. 0.] # one_hot(1) [0. 0. 0. 1.]]], requires_grad=False, dtype=float64) # one_hot(3)
  >>> a = tce.Tensor([0, 1, 2], requires_grad = True, dtype = tce.int32)
  >>> depth = 3
  >>> tce.one_hot(a, depth, on_value = 6, off_value = 9)  # output: (3, 3)
  >>> Tensor(
  >>> [[6. 9. 9.]  # one_hot(0), on_value = 6, off_value = 9
  >>>  [9. 6. 9.]  # one_hot(1), on_value = 6, off_value = 9
  >>>  [9. 9. 6.]], requires_grad=False, dtype=float64) # one_hot(2), on_value = 6, off_value = 9

- `ELU`:

  Here implements the ELU(Exponential Linear Unit) function for tensors.
  Parameter: t: 'Tensorable'
  alpha: optional: 'Float'
  Return: Tensor: ELU(t) = t, if t > 0
  = alpha*(tce.exp(t)-1), if t < 0
  Examples: >>> a = tce.Tensor([-1, 1], requires_grad = True, dtype = tce.int32) >>> b = tce.ELU(a) >>> b Tensor( [-0.63212056 1. ], requires_grad=True, dtype=float64)
  >>> a = tce.Tensor([-1, 1], requires_grad = True, dtype = tce.int32)
  >>> b = tce.ELU(a, alpha = 2)
  >>> b
  >>> Tensor(
  >>> [-1.26424112  1.        ], requires_grad=True, dtype=float64)

- `ReLUx`:

  Here implements the ReLUx function for tensors.
  Parameter: t: 'Tensorable'
  x: 'Float'
  Return: Tensor: value of ReLUx(t) = min(max(0, t), x)
  Examples: >>> a = tce.Tensor([[-1, 1],
   [7, 9]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.ReLUx(a)
  >>> b
  >>> Tensor(
  >>> [[0. 1.]
  >>>  [7. 8.]], requires_grad=True, dtype=float64)
  >>> y = tce.trange(6, 10)
  >>> print(tce.ReLUx(y))
  >>> Tensor(
  >>> [6. 7. 8. 8.], requires_grad=False, dtype=float64)

- `abs`:

  Here implements the abs(absolute) function for tensors.
  Parameter: t: 'Tensorable'
  isnew: 'Bool': decides whether giving result gradient
  Return: Tensor: value of abs(t) = t, if t > 0
  = -t, if t < 0
  Example: >>> a = tce.Tensor([[-1, 1],
   [-6, 9]], requires_grad = True, dtype = tce.int32)
  >>> b = tce.abs(a)
  >>> b
  >>> Tensor(
  >>> [[1 1]
  >>>  [6 9]], requires_grad=False, dtype=int32)

## Neural Net 

In `Trchime`, there are two ways to  construct models. One is sequence. Another is customizing. 

### module

`compile`:

- Notes: You can use the compile function to configure your model's optimizer parameters,
  loss function, learning rate, etc.

- Parameters: optimizer: a string or a custom optimizer. You can choose an optimizer or

  define a new optimizer for you model.
  Trchime provides five optimizers for your training. You can pass strings of five enum types in tce.nn package as arguments. 1. SGD_OPTIMIZER: Stochastic Gradient Descent Optimizer 2. SGDM_OPTIMIZER: MomentumOptimizer: 3. ADAGRAD_OPTIMIZER: Adaptive Gradient Optimizer 4. RMSPROP_OPTIMIZER: Root Mean Square Prop Optimizer 5. ADAM_OPTIMIZER: Adaptive Moment Estimation Optimizer
  You can also customize your own optimizer, which requires you to implement a class that inherits from Optimizer in the tce.nn package. You are required to override __init__ and step method of parent class.

- loss: a string or a custom loss. You can choose an loss function or define a new
    loss function for your model.
    Trchime provides three loss function for your training. You can pass strings of three enums types in tce.nn package as arguments. 1. MSELOSS: mean_square_error_loss 1. CATEGORYLOSS: categorical_cross_entropy_loss 3. MAELOSS: mean_absolute_error_loss
    
- **kwargs: Optional, You can configure some optional parameters of the optimizer,
    such as learning_rate, sgdm_beta, adam_corrected, etc. If not given, the optimizer will be initialized with default parameters. You can assign values to the following optional parameters: 1. learning_rate: float, default value is 0.01 2. sgm_beta: float, default value is 0.9 3. rmsprop_beta: float, default value is 0.9 4. adam_beta1: float, default value is 0.9 5. adam_beta2: float, default value is 0.9 6. adam_corrected: boolean, default value is False
    
    examples:
    
    ```python
    model = MyModel()  # instantiate your model
            model.compile(optimizer = tce.nn.SGD_OPTIMIZER, 
                          # choose stochastic gradient descent optimizer
                          loss = tce.nn.MSELOSS,  # set mean square loss function
                          learning_rate = 0.1)  # set learning rate
    ```
    
    ```python
    class MyOptimizer(tce.nn.Optimizer):
    
                def __init__(self):
                    super().__init__('my optimizer') 
                    # define the name of the new-defined optimizer
                    self.lr = 0.1  # set learning rate
    
                def step(self, module):
                    for parameter in module.parameters():
                        parameter.assign_sub(self.lr * parameter.grad)
                        # define the learning way, here is just the gradient descent
    
            model = MyModel()  # instantiate your model
            my_op = MyOptimizer  # instantiate your optimizer
    
            model.compile(optimizer = my_op,  # replace with the new optimizer
                          loss = tce.nn.MSELOSS)
    ```
    
    ```python
    model = MyModel()
            model.compile(optimizer=tce.nn.ADAM_OPTIMIZER,  
                          # choose Adaptive Moment Estimation Optimizer
                          loss=tce.nn.CATEGORYLOSS,  
                          # choose categorical cross entropy loss function
                          learning_rate=0.2,  # set learnin_rate
                          adam_beta1 = 0.95,
                          adam_corrected = True)  
        					# change default arguments for adam optimizer
    ```
    
    ```python
    class MyLoss(tce.nn.loss.LOSS):
    
                def __init__(self):
                    super().__init__("my loss")  # define the name of loss function
    
                def define_loss(self, predicted: 'Tensor', actual: 'Tensor') -> None:
                    '''
                    Overrider this method to define your loss function.
                    You are required to declare `self.loss`.
    
                    Parameters:
                    predicted: The predicted output
                    actual: The actual output
                    model: training model
    
                    You can get information to define you loss function from above three parameters.
    
                    For examples, your can implements square sum error loss as:
                        >>> self.loss = ((predicted - actual) ** 2).sum()
    
                    '''
    
                    self.loss  = ((predicted-actual)**2).sum()  
                    # define square sum error loss
    
                    for parameter in model.parameters():
                        # model.parameters would return an iterator which contain all trainable parameters in your model
                        self.loss += (parameter ** 2).sum()
                        # implements L1 regularization for your model
    
            model = MyModel()  # instantiate your model
            my_loss = MyLoss()  # instantiate your loss class
    
            model.compile(optimizer=tce.nn.ADAM_OPTIMIZER,
                          loss=my_loss,  # replace with new loss
                          learning_rate=0.1)
    ```
    
    Also see:
    
    [examples](examples/) 

`fit`

- Notes: our model would be trained from here.

- Parameters: 

  x: 'Tensorable', The input of the training set
  y: Tensorable', The output of the training set
  batch_size：optional, 'Integer', batch size.
  epochs: optional, 'Integer', The times of iteration. If not given, default value is 1
  validation_split: optional, 'Float', The proportion of the training set chosen as test set.
  validation_data: optional, 'Tensorable', The test set for neural network which has been trained
  shuffle: optional, 'Bool', Whether to shuffle the inout of training set and test set.
  If not given, default value is True
  validation_freq: optional, 'Integer', The frequency of showing the accuracy of per epoch.
  If not given, default value is 1.
  show_acc_tr: optional, 'Bool', Whether to show the accuracy of train set of per epoch. Default value is False.
  show_acc: optional, 'Bool', Whether to show the accuracy of test set of per epoch. Default value is False.
  show_loss: optional, 'Bool', Whether to show the loss of test set of per epoch. Default value is False.
  epochs_mean_loss: optional, 'Bool', Whether to show the average loss of epochs. Default value is False.
  valid_inputs: optional, 'Bool', default value is False.
  Whether invalidate your input training set. Please ensure the predict method of your model never uses the arguments inputs before you assign valid_inputs with True.
  show_batch_loss: optional, 'Bool', default value is False. Whether show loss per batch.
  show_batch_acc: optional, 'Bool', default value is False. Whether show accuracy per batch.
  accuracy_show: optional, 'AccuracyBoard', default value is MCA_BOARD.
  Trchime will treat your model as a multi-class network by default and calculate the accuracy of your model output. You can also customize the calculation method of the accuracy of your model output.

- Notes: 

  1. Generally, you only need to assign value to anyone of validation_split or validation_data
     If your assign values to both of them, only validation_data would work.'

  2. The frequency of showing accuracy of test set up to once a epoch.
  3. Please ensure the predict method of your model never uses the arguments inputs before you assign valid_inputs with True.
  4. More details about customizing your own accuracy's calculation please see: trchime/examples/xxxx.py

- Example: 
  
  ```python
  model.fit(x, y,  # input training data
                    batch_size = 32,  # set batch_size and epochs
                    epochs = 100,
                    validation_split = 0.2,  # split 20% of training set as test set
                    show_acc = True)  # show accuracy per epoch
  ```
  
- More examples please see [examples](examples/)

`add`:

- Here implements the add function for module. This is how the sequence constructs the model. You can build a network by adding a forward propagation layer to your network through the add method.

- layer: ['ConnectionLayer'](trchime/nn/layer.py): You can define number of neuron in this layer and the activation function

  Trchime provides some computing layers: 

  1.Dense: 
  2.Batch_normalize_layer: 
  3.Flatten: 
  4.Convolution_layer: 
  5.MaxPooling_layer: 
  6.AveragePool_layer:

  Trchime provides some activation function for computing layer in enum class Activation: 
  1.Activation.TANH_ACTIVATION: hyperbolic Tangent activation function   
  2.Activation.SIGMOID_ACTIVATION: sigmoid activation function 
  3.Activation.RELU_ACTIVATION: rectified linear unit activation function 
  4.Activation.LEAKY_RELU_ACTIVATION: leaky rectified linear unit activation function 
  5.Activation.SOFTPLUS_ACTIVATION: softplus activation function 
  6.Activation.SOFTMAX_ACTIVATION: softmax activation function 
  7.Activation.ELU_ACTIVATION: exponential linear units activation function 
  8.Activation.RELUX_ACTIVATION: rectified linear unit x activation function 
  9.Activation.NONE: set no activation function

- example

  ```python
  model = tce.Module()
              model.init()
              model.add(tce.nn.Dense(nums = 32,  # number of the neuron
                                     activation = tce.nn.Activation.RELU_ACTIVATION))  # define relu activation function
  
              model.add(tce.nn.Dense(nums = 4,  # number of the neuron
                                     activation = tce.nn.Activation.SOFTMAX_ACTIVATION))  # define softmax activation function
  ```

- more example

  [4x32x24x2network](examples\4x16x16x2 network.py)

build 4x16x16x2 network:

```python
import trchime as tce


def get_dataset(p_sum):
    """
    Produce dataset for 2 classification problem.
    Generate some points in 4-dimensional space and distribute them
    to 2 labels.
    if points are in unit boll of 4-dimension space, they would
    be marked as label 1.
    If not, they would be marked as label 2
    """
    x_train = 2 * (tce.random.random((p_sum, 4)) - 0.5)
    y_train = tce.zeros((p_sum, 2))

    for i in range(p_sum):
        if (tce.sum(x_train[i, :] ** 2) < 1).data:  # in -> labels: [0, 1]
            y_train[i, 0] = 0
            y_train[i, 1] = 1
        else:  # out -> labels: [1, 0]
            y_train[i, 0] = 1
            y_train[i, 1] = 0

    return x_train, y_train

# construct your model by inherit class `tce.Module`
# you need to overwrite two methods
class unit_Model(tce.Module):
    def __init__(self):
        self.init()  # It's necessary to call `init` method when extend Module
        # initialize paramters' matrix randomly
        self.w1 = tce.random.randn(4, 16, requires_grad = True) * 0.1
        self.b1 = tce.random.randn(1, 16, requires_grad = True) * 0.1

        self.w2 = tce.random.randn(16, 16, requires_grad = True) * 0.1
        self.b2 = tce.random.randn(1, 16, requires_grad = True) * 0.1

        self.w3 = tce.random.randn(16, 2, requires_grad = True) * 0.1
        self.b3 = tce.random.randn(1, 2, requires_grad = True) * 0.1

    def predict(self, inputs):
        # define your forward model here
        # 4x16x16x2
        z1 = inputs @ self.w1 + self.b1
        a1 = tce.ReLU(z1)

        z2 = a1 @ self.w2 + self.b2
        a2 = tce.ReLU(z2)

        z3 = a2 @ self.w3 + self.b3
        return tce.sigmoid(z3)  # output layer use sigmoid activate function

# collect dataset
points_sum = 1000
x, y = get_dataset(points_sum)

# instantiate your model
model = unit_Model()

# compile your model
model.compile(optimizer = tce.nn.ADAM_OPTIMIZER,  # choose stochastic gradient descent optimizer
              loss = tce.nn.MSELOSS,  # set mean square loss function
              learning_rate = 0.1)  # set learning rate

# train your model
model.fit(x, y,  # input training data
          batch_size = 32,  # set batch_size and epochs
          epochs = 100,
          validation_split = 0.2,  # split 20% of trainingset as testset
          show_acc = True)  # show accuracy per epoch

```

build 4x32x24x2 network in sequence:

```python
import trchime as tce
import os

def get_dataset(p_sum):
    """
    Produce dataset for 2 classification problem.
    Generate some points in 4-dimensional space and distribute them
    to 2 labels.
    if points are in unit boll of 4-dimension space, they would
    be marked as label 1.
    If not, they would be marked as label 2
    """
    x_train = 5 * (tce.random.random((p_sum, 4)) - 0.5)
    y_train = tce.zeros((p_sum, 2))

    for i in range(p_sum):
        if (tce.sum(x_train[i, :] ** 2) < 1).data:  # in -> labels: [0, 1]
            y_train[i, 0] = 0
            y_train[i, 1] = 1
        else:  # out -> labels: [1, 0]
            y_train[i, 0] = 1
            y_train[i, 1] = 0

    return x_train, y_train

# collect dataset
points_sum = 2000
x, y = get_dataset(points_sum)

model_save_path ='4x32x24x2network.pkl'

if os.path.exists(model_save_path):
    model = tce.loadmodel(model_save_path)
else:
    # instantiate your model
    model = tce.Module()
    # add some computing layer for your model
    model.add(tce.nn.Dense(nums = 32,
                           activation = tce.nn.Activation.RELU_ACTIVATION))
    model.add(tce.nn.Dense(nums = 24,
                           activation = tce.nn.Activation.RELU_ACTIVATION))
    model.add(tce.nn.Dense(nums = 2,
                           activation = tce.nn.Activation.SIGMOID_ACTIVATION))

# compile your model
model.compile(optimizer = tce.nn.ADAM_OPTIMIZER, # choose adam optimizer 
              loss = tce.nn.MSELOSS)  # set mean square loss function

# train your model
model.fit(x, y,
          batch_size = 32,
          epochs = 50,
          validation_split = 0.3,  # split 30% of trainingsets as testset
          show_acc_tr = True)

tce.savemodel(model, url = model_save_path)
```

